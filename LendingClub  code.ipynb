{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LendingClub -classification(purpose of new client loan status predict 1 은 미래예측이 가능하나 bad loan 진단력이 약하고 predict 2는 상환률을 변수로 사용하여 분류정확도가 완벽하나 미래의 상환정도를 예측할수가 없다. 여기선 na가 많아 삭제된 변수중 bad loan에 관련된 변수를 더 활용할 방법을 찾던지 아니면 bad loan 데이터가 너무 작으므로 늘리는 방법을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### current 상태를 제외하고 good,bad 확정인 과거데이터를 학습하여 분류기준을 찾는 모델이다. 그렇기 때문에 'repayment_rate'( total_pymnt / funded_amnt) 성격이 결과변수인 성격이 있어서 제외하였다. 정확도는 상대적으로 떨어져도 의미있는 분석모델.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries we are going to use for our data analysis.\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotly visualizations\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "# plotly.tools.set_credentials_file(username='AlexanderBach', api_key='o4fx6i1MtEIJQxfWYvU1')\n",
    "\n",
    "\n",
    "# For oversampling Library (Dealing with Imbalanced Datasets)\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Other Libraries\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import numpy as np\n",
    "# import pandas as pd \n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn.linear_model as linear_model\n",
    "# import seaborn as sns\n",
    "#import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./LoanStats3a.csv')\n",
    "df = pd.read_csv('./loan.csv')\n",
    "df_origin=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_description = pd.read_excel('./LCDataDictionary.xlsx').dropna()\n",
    "df_description.style.set_properties(subset=['Description'], **{'width': '1000px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the Null values\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('dti_joint', 'annual_inc_joint', 'il_util', 'mths_since_rcnt_il', 'open_acc_6m', 'open_il_6m', 'open_il_12m',\n",
    "           'open_il_24m', 'inq_last_12m', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',\n",
    "           'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'total_bal_il', 'tot_coll_amt',\n",
    "           'tot_cur_bal', 'total_rev_hi_lim', 'revol_util', 'collections_12_mths_ex_med', 'open_acc', 'inq_last_6mths',\n",
    "           'verification_status_joint', 'acc_now_delinq'):\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id제거\n",
    "df=df.iloc[:,2:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "perc = df.isnull().sum()/df.isnull().count() *100\n",
    "perc1 = (round(perc,2).sort_values(ascending=False))\n",
    "\n",
    "# Creating a data frame:\n",
    "df_miss = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis = df_miss[df_miss[\"Percentage %\"]>0]\n",
    "top_mis.reset_index(inplace=True)\n",
    "top_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Distributions:\n",
    "We will start by exploring the distribution of the loan amounts and see when did the loan amount issued increased significantly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we need to know:\n",
    "\n",
    "Understand what amount was mostly issued to borrowers.\n",
    "Which year issued the most loans.\n",
    "The distribution of loan amounts is a multinomial distribution ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "Most of the loans issued were in the range of 10,000 to 20,000 USD.\n",
    "The year of 2015 was the year were most loans were issued.\n",
    "Loans were issued in an incremental manner. (Possible due to a recovery in the U.S economy)\n",
    "The loans applied by potential borrowers, the amount issued to the borrowers and the amount funded by investors are similarly distributed, meaning that Only controlled data set were released. If there is a loan value that is different from the loaner's demand, It will be able to predict the loan approval requirements of the company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.distplot(df[\"loan_amnt\"])\n",
    "g.set_xlabel(\"\", fontsize=12)\n",
    "g.set_ylabel(\"Frequency Dist\", fontsize=12)\n",
    "g.set_title(\"Frequency Distribuition\", fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "g1 = sns.violinplot(y=\"loan_amnt\", data=df, \n",
    "               inner=\"quartile\", palette=\"hls\")\n",
    "g1.set_xlabel(\"\", fontsize=12)\n",
    "g1.set_ylabel(\"Amount Dist\", fontsize=12)\n",
    "g1.set_title(\"Amount Distribuition\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#While the histogram method creates discontinuities by increasing the value of bin corresponding to each data in discrete, the KDE (Kernel Density Estimation) method adds each data as a function of the kernel, showing a smooth probability density function (pdf) as shown in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16,5))\n",
    "\n",
    "sns.distplot(df[\"loan_amnt\"], ax=ax[0], color=\"#F7522F\")\n",
    "ax[0].set_title(\"Loan Applied by the Borrower\", fontsize=14)\n",
    "sns.distplot(df[\"funded_amnt\"], ax=ax[1], color=\"#2F8FF7\")\n",
    "ax[1].set_title(\"Amount Funded by the Lender\", fontsize=14)\n",
    "sns.distplot(df[\"funded_amnt_inv\"], ax=ax[2], color=\"#2EAD46\")\n",
    "ax[2].set_title(\"Total committed by Investors\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year,month 분리\n",
    "df['issue_month'], df['issue_year'] = df['issue_d'].str.split('-', 1).str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_order = [\"Jan\", \"Feb\", \"Mar\", \"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "df['issue_month'] = pd.Categorical(df['issue_month'],categories=months_order, ordered=True)\n",
    "\n",
    "#Issue_d x loan_amount\n",
    "plt.figure(figsize = (14,10))\n",
    "\n",
    "g = sns.countplot(x='issue_month', \n",
    "                  data=df, \n",
    "                  hue='loan_status')\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "g.set_xlabel(\"Months\", fontsize=15)\n",
    "g.set_ylabel(\"count\", fontsize=15)\n",
    "g.legend(loc='best')\n",
    "g.set_title(\"Loan status by Months\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,6))\n",
    "#Looking the count of defaults though the issue_d that is The month which the loan was funded\n",
    "g = sns.countplot(x='issue_year', data=df,\n",
    "                  hue='loan_status')\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "g.set_xlabel(\"years\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "g.legend(loc='upper left')\n",
    "g.set_title(\"Analysing Loan Status by Years\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The year of 2015 was the year were the highest amount of loans were issued \n",
    "# This is an indication that the economy is quiet recovering itself.\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(df['issue_year'], df['loan_amnt'], data=df, palette='tab10')\n",
    "plt.title('Issuance of Loans', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Average loan amount issued', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Loans vs Bad Loans: Types of Loans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Loans:\n",
    " In this section, we will see what is the amount of bad loans Lending Club has declared so far, of course we have to understand that there are still loans that are at a risk of defaulting in the future.\n",
    "What we need to know:\n",
    "The amount of bad loans could increment as the days pass by, since we still have a great amount of current loans.\n",
    "Average annual income is an important key metric for finding possible opportunities of investments in a specific region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "Currently, bad loans consist 7.60% of total loans but remember that we still have current loans which have the risk of becoming bad loans. (So this percentage is subjected to possible changes.)\n",
    "The NorthEast region seems to be the most attractive in term of funding loans to borrowers.\n",
    "The SouthWest and West regions have experienced a slight increase in the \"median income\" in the past years.\n",
    "Average interest rates have declined since 2012 but this might explain the increase in the volume of loans.\n",
    "Employment Length tends to be greater in the regions of the SouthWest and West\n",
    "Clients located in the regions of NorthEast and MidWest have not experienced a drastic increase in debt-to-income(dti) as compared to the other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ax = sns.countplot(y=df['loan_status'],order = df['loan_status'].value_counts().index)\n",
    "ax = plt.title('Loan Status Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan Distribution by Status\n",
    "\n",
    "Below are the definitions of each loan status:\n",
    "\n",
    "Issued: New loan that has passed all LendingClub reviews, received full funding, and has been issued.\n",
    "\n",
    "Current: Loan is up to date on all outstanding payments.\n",
    "\n",
    "In Grace Period: Loan is past due but within the 15-day grace period.\n",
    "\n",
    "Late (16-30): Loan has not been current for 16 to 30 days.\n",
    "\n",
    "Late (31-120): Loan has not been current for 31 to 120 days.\n",
    "\n",
    "Fully paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.\n",
    "\n",
    "Default: Loan has not been current for an extended period of time.\n",
    "\n",
    "Charged Off: Loan for which there is no longer a reasonable expectation of further payments. Upon Charge Off, the remaining \n",
    "             principal balance of the Note is deducted from the account balance.\n",
    "\n",
    "More status definition : LendingClub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the loans that are bad from loan_status column\n",
    "\n",
    "good_loan = [\"Fully Paid\", \"Does not meet the credit policy. Status:Fully Paid\"] \n",
    "unknown= [\"Current\" , \"Issued\"]          \n",
    "\n",
    "\n",
    "df['loan_condition'] = np.nan\n",
    "\n",
    "def loan_condition(status):\n",
    "    if status in good_loan:\n",
    "        return 'Good Loan'\n",
    "    elif status in unknown:\n",
    "        return 'Current'\n",
    "    else:\n",
    "        return 'Bad Loan'\n",
    "    \n",
    "    \n",
    "df['loan_condition'] = df['loan_status'].apply(loan_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=(18,8))\n",
    "\n",
    "colors = [\"#3ebfe2\",\"#3791D7\",\"#D72626\" ]\n",
    "labels = [\"Current\",\"Good Loans\", \"Bad Loans\"]\n",
    "\n",
    "plt.suptitle('Information on Loan Conditions', fontsize=18)\n",
    "\n",
    "df[\"loan_condition\"].value_counts().plot.pie(explode=[0,0,0.2], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n",
    "                                             labels=labels, fontsize=10, startangle=20)\n",
    "\n",
    "\n",
    "# ax[0].set_title('State of Loan', fontsize=16)\n",
    "ax[0].set_ylabel('% of Condition of Loans', fontsize=10)\n",
    "\n",
    "# sns.countplot('loan_condition', data=df, ax=ax[1], palette=colors)\n",
    "# ax[1].set_title('Condition of Loans', fontsize=20)\n",
    "# ax[1].set_xticklabels(['Good', 'Bad'], rotation='horizontal')\n",
    "palette = [\"#3791D7\",\"#D72626\",\"#3ebfe2\"]\n",
    "\n",
    "sns.barplot(x=df['issue_year'], y=df['loan_amnt'], hue=\"loan_condition\", data=df, palette=palette, estimator=lambda x: len(x) / len(df) * 100)\n",
    "ax[1].set(ylabel=\"(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loans Issued by Region and purpose\n",
    "In this section we want to analyze loans issued by region in order to see region patters that will allow us to understand which region gives Lending Club.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: \n",
    "SouthEast , West and NorthEast regions had the high amount of loans issued while SouthWest and MidEast region has the lower.\n",
    "\n",
    "The most common reason for a loan request is debt consolidation, followed by credit card and home improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['addr_state'].unique()\n",
    "\n",
    "# Make a list with each of the regions by state.\n",
    "\n",
    "west = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\n",
    "south_west = ['AZ', 'TX', 'NM', 'OK']\n",
    "south_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\n",
    "mid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\n",
    "north_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n",
    "\n",
    "df['region'] = np.nan\n",
    "\n",
    "def finding_regions(state):\n",
    "    if state in west:\n",
    "        return 'West'\n",
    "    elif state in south_west:\n",
    "        return 'SouthWest'\n",
    "    elif state in south_east:\n",
    "        return 'SouthEast'\n",
    "    elif state in mid_west:\n",
    "        return 'MidWest'\n",
    "    elif state in north_east:\n",
    "        return 'NorthEast'\n",
    "    \n",
    "\n",
    "\n",
    "df['region'] = df['addr_state'].apply(finding_regions)\n",
    "#df['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같은 df['region']결과값 다른코드\n",
    "\n",
    "# regions_dict = pd.Series(df['region'].values,index=df['addr_state']).to_dict()\n",
    "# df['region'] = df['addr_state'].map(regions_dict)\n",
    "# df['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'region' by mapping the state value to region from the region dict\n",
    "\n",
    "\n",
    "region_dist = df.groupby(['region']).size()\n",
    "region_dist \n",
    "explode = (0, 0, 0, 0, 0) \n",
    "colors = [\"#efc186\",\"#a6ddb5\",\"#dda19b\",\"#9ebfe2\",\"#3ebfe2\"]\n",
    "plt.figure(figsize=(15,6))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1 = plt.pie(region_dist,autopct='%1.1f%%',explode=explode, labels=region_dist.index,colors = colors)\n",
    "ax1 = plt.title('Loan Distribution By Region')\n",
    "ax1 = plt.xlabel('Region')\n",
    "ax1 = plt.axis('equal')  \n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2 = sns.countplot(y=df['purpose'],order = df['purpose'].value_counts().index)\n",
    "ax2 = plt.title('Loan Purpose Distribution')\n",
    "ax2 = plt.ylabel('Loan Purposes')\n",
    "ax2 = plt.xlabel('Frequency')\n",
    "ax2 = plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 67429 loans categorized as bad loans\n",
    "badloans_df = df.loc[df[\"loan_condition\"] == \"Bad Loan\"]\n",
    "\n",
    "# loan_status cross\n",
    "loan_status_cross = pd.crosstab(badloans_df['region'], badloans_df['loan_status']).apply(lambda x: x/x.sum() * 100)\n",
    "number_of_loanstatus = pd.crosstab(badloans_df['region'], badloans_df['loan_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off = loan_status_cross['Charged Off'].values.tolist()\n",
    "default = loan_status_cross['Default'].values.tolist()\n",
    "not_meet_credit = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].values.tolist()\n",
    "grace_period = loan_status_cross['In Grace Period'].values.tolist()\n",
    "short_pay = loan_status_cross['Late (16-30 days)'] .values.tolist()\n",
    "long_pay = loan_status_cross['Late (31-120 days)'].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "charged = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y= charged_off,\n",
    "    name='Charged Off',\n",
    "    marker=dict(\n",
    "        color='rgb(192, 148, 246)'\n",
    "    ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "defaults = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y=default,\n",
    "    name='Defaults',\n",
    "    marker=dict(\n",
    "        color='rgb(176, 26, 26)'\n",
    "    ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "credit_policy = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y= not_meet_credit,\n",
    "    name='Does not meet Credit Policy',\n",
    "    marker = dict(\n",
    "        color='rgb(229, 121, 36)'\n",
    "    ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "grace = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y= grace_period,\n",
    "    name='Grace Period',\n",
    "    marker = dict(\n",
    "        color='rgb(147, 147, 147)'\n",
    "    ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "short_pays = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y= short_pay,\n",
    "    name='Late Payment (16-30 days)', \n",
    "    marker = dict(\n",
    "        color='rgb(246, 157, 135)'\n",
    "    ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "long_pays = go.Bar(\n",
    "    x=['MidWest', 'NorthEast', 'SouthEast', 'SouthWest', 'West'],\n",
    "    y= long_pay,\n",
    "    name='Late Payment (31-120 days)',\n",
    "    marker = dict(\n",
    "        color = 'rgb(238, 76, 73)'\n",
    "        ),\n",
    "    text = '%'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [charged, defaults, credit_policy, grace, short_pays, long_pays]\n",
    "layout = go.Layout(\n",
    "    barmode='stack',\n",
    "    title = '% of Bad Loan Status by Region',\n",
    "    xaxis=dict(title='US Regions')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how many loans were issued by creditscore\n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2)\n",
    "cmap = plt.cm.coolwarm\n",
    "\n",
    "by_credit_score = df.groupby(['issue_year', 'grade']).loan_amnt.mean()\n",
    "by_credit_score.unstack().plot(legend=False, ax=ax1, figsize=(14, 4), colormap=cmap)\n",
    "ax1.set_title('Loans issued by Credit Score', fontsize=14)\n",
    "    \n",
    "    \n",
    "by_inc = df.groupby(['issue_year', 'grade']).int_rate.mean()\n",
    "by_inc.unstack().plot(ax=ax2, figsize=(14, 4), colormap=cmap)\n",
    "ax2.set_title('Interest Rates by Credit Score', fontsize=14)\n",
    "\n",
    "ax2.legend(bbox_to_anchor=(-1.0, -0.3, 1.7, 0.1), loc=5, prop={'size':12},\n",
    "           ncol=7, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(212)\n",
    "\n",
    "cmap = plt.cm.coolwarm_r\n",
    "\n",
    "loans_by_region = df.groupby(['grade', 'loan_condition']).size()\n",
    "loans_by_region.unstack().plot(kind='bar', stacked=True, colormap=cmap, ax=ax1, grid=False)\n",
    "ax1.set_title('Type of Loans by Grade', fontsize=14)\n",
    "\n",
    "\n",
    "loans_by_grade = df.groupby(['sub_grade', 'loan_condition']).size()\n",
    "loans_by_grade.unstack().plot(kind='bar', stacked=True, colormap=cmap, ax=ax2, grid=False)\n",
    "ax2.set_title('Type of Loans by Sub-Grade', fontsize=14)\n",
    "\n",
    "by_interest = df.groupby(['issue_year', 'loan_condition']).int_rate.mean()\n",
    "by_interest.unstack().plot(ax=ax3, colormap=cmap)\n",
    "ax3.set_title('Average Interest rate by Loan Condition', fontsize=14)\n",
    "ax3.set_ylabel('Interest Rate (%)', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average interest is 13.25% Anything above this will be considered of high risk let's see if this is true.\n",
    "df['interest_payments'] = np.nan\n",
    "lst = [df]\n",
    "\n",
    "for col in lst:\n",
    "    col.loc[col['int_rate'] <= 13, 'interest_payments'] = 'Low'\n",
    "    col.loc[col['int_rate'] > 13, 'interest_payments'] = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "palette = ['#009393', '#930000']\n",
    "plt.subplot(221)\n",
    "ax = sns.countplot(x='interest_payments', data=df, \n",
    "                  palette=palette, hue='loan_condition')\n",
    "\n",
    "ax.set_title('The impact of interest rate \\n on the condition of the loan', fontsize=14)\n",
    "ax.set_xlabel('Level of Interest Payments', fontsize=12)\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.subplot(222)\n",
    "ax1 = sns.countplot(x='interest_payments', data=df, \n",
    "                   palette=palette, hue='term')\n",
    "\n",
    "ax1.set_title('The impact of maturity date \\n on interest rates', fontsize=14)\n",
    "ax1.set_xlabel('Level of Interest Payments', fontsize=12)\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "low = df['loan_amnt'].loc[df['interest_payments'] == 'Low'].values\n",
    "high = df['loan_amnt'].loc[df['interest_payments'] == 'High'].values\n",
    "\n",
    "\n",
    "ax2= sns.distplot(low, color='#009393', label='Low Interest Payments', fit=norm, fit_kws={\"color\":\"#483d8b\"}) # Dark Blue Norm Color\n",
    "ax3 = sns.distplot(high, color='#930000', label='High Interest Payments', fit=norm, fit_kws={\"color\":\"#c71585\"}) #  Red Norm Color\n",
    "plt.axis([0, 36000, 0, 0.00016])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 80% of missing values\n",
    "na_thresh = len(df)*80/100\n",
    "df = df.dropna(thresh=na_thresh, axis=1)\n",
    "\n",
    "# Check the new dimension\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank -> na\n",
    "df=df.replace('', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na 제거  55개 독립변수에서 34개됨.\n",
    "#df.dropna(axis='index', how='any')\n",
    "df=df.dropna(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_miss_tot = df.isnull().sum().sort_values(ascending = False)\n",
    "n_perc = df.isnull().sum()/df.isnull().count()*100\n",
    "n_perc1 = (round(n_perc, 2).sort_values(ascending = False))\n",
    "n_miss_df = pd.concat([n_miss_tot, n_perc1], axis = 1, keys=([\"total\", \"Percentage\"])).sort_values(\n",
    "    by =\"Percentage\", ascending = False).head(8)\n",
    "n_miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists number of unique values by each column\n",
    "df.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the columns which has single(unique) values like - NA, 0, 'f', 'n' etc.\n",
    "unique = df.nunique()\n",
    "unique = unique[unique.values == 1]\n",
    "unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the columns which has single(unique) values like - NA, 0, 'f', 'n' etc.\n",
    "df.drop(labels = list(unique.index), axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['int_rate'].round(0).astype(int)\n",
    "\n",
    "# plt.figure(figsize = (10,8))\n",
    "\n",
    "# #Exploring the Int_rate\n",
    "# plt.subplot(211)\n",
    "# g = sns.distplot(np.log(df[\"int_rate\"]))\n",
    "# g.set_xlabel(\"\", fontsize=12)\n",
    "# g.set_ylabel(\"Distribuition\", fontsize=12)\n",
    "# g.set_title(\"Int Rate Log distribuition\", fontsize=20)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# g1 = sns.countplot(x=df['int_rate'].round(0).astype(int),data=df, \n",
    "#                    palette=\"Set2\")\n",
    "# g1.set_xlabel(\"Int Rate\", fontsize=12)\n",
    "# g1.set_ylabel(\"Count\", fontsize=12)\n",
    "# g1.set_title(\"Int Rate Distribuition\", fontsize=20)\n",
    "\n",
    "# plt.subplots_adjust(wspace = 0.2, hspace = 0.6,top = 0.9)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py \n",
    "import plotly.figure_factory as ff \n",
    "import plotly.graph_objs as go \n",
    "from plotly import tools \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot \n",
    "py.init_notebook_mode(connected= True)\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in df[\"loan_status\"].unique():\n",
    "    data.append(go.Box(y = df[df[\"loan_status\"]==i][\"int_rate\"], name = i))\n",
    "    \n",
    "layout = go.Layout(title = 'Interested Rate based on Loan Status', \n",
    "                   xaxis = dict(title = 'Status'), \n",
    "                   yaxis = dict(title = 'Interest Rate'))\n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= []\n",
    "for i in df[\"grade\"].unique():\n",
    "    data1.append(go.Box(y = df[df[\"grade\"]==i][\"int_rate\"], name = i))\n",
    "    \n",
    "layout = go.Layout(title = 'Interested Rate based on Grades', \n",
    "                   xaxis = dict(title = 'Grades'), \n",
    "                   yaxis = dict(title = 'Interest Rate'))\n",
    "fig = dict(data = data1, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= []\n",
    "for i in df[\"emp_length\"].unique():\n",
    "    data2.append(go.Box(y = df[df[\"emp_length\"]==i][\"int_rate\"], name = i))\n",
    "    \n",
    "layout = go.Layout(title = 'int_rate on emp_length', \n",
    "                   xaxis = dict(title = 'emp_length'), \n",
    "                   yaxis = dict(title = 'int_rate'))\n",
    "fig = dict(data = data2, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3= []\n",
    "for i in df[\"purpose\"].unique():\n",
    "    data3.append(go.Box(y = df[df[\"purpose\"]==i]['funded_amnt'], name = i))\n",
    "    \n",
    "layout = go.Layout(title = 'funded_amnt on purpose', \n",
    "                   xaxis = dict(title = 'purpose'), \n",
    "                   yaxis = dict(title = 'funded_amnt'))\n",
    "fig = dict(data = data3, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # percentage to numeric\n",
    "# df['int_rate'] = pd.to_numeric(df['int_rate'].str.strip('%'),errors='coerce')\n",
    "# #df['int_rate'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['int_rate']로 credit_score종속변수 만들기\n",
    "\n",
    "df['credit_score']=round(np.reciprocal(df['int_rate'])*5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"sub_grade\",\"int_rate\",\"credit_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8)) \n",
    "plt.figure(1)\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "\n",
    "1.데이터 na 값,즉 결측치가 80%이상인 행 지운다\n",
    "\n",
    "2.데이터를 continuous data(float,int) 와 categorical data로 나눈다\n",
    "\n",
    "3.연속형데이터:\n",
    "\n",
    "  1)상관관계 분석: 다중공선성 제거.상호 상관관계 높은 변수 통합(통합변수 만들거나 pca등으로 차원감소)하거나 삭제.\n",
    "    연속형변수의 분산확인: 분산이 0에 가까운 변수 삭제\n",
    "    \n",
    "  2)결측치는 중앙값이나 평균등 논리적 의미 있는 값으로 대체.(의미 찾기어려우면 클러스터링후 대표값)\n",
    "   이상치는 IQR이나 왜도를 측정하여 이상치 체크하여 이상치 제거할수 있으나 이상치의 숨은 의미를 찾는것이 중요.\n",
    "   예를들어 연봉이 높은 이상치일 경우 전문직일수 있으니 전문직 유무의 변수를 만드는것이 더 정확하다. 그러므로 이상치도 \n",
    "   최대한 유지하며 제거없이 스케일 표준화가 낫다. 결측치 대체를 못한 행은 지운다.\n",
    "   \n",
    "4.카데고리형:\n",
    "\n",
    "  범주의 개수(number of unique values)가 1개인 변수는 지운다. 아무런 분포가 없으므로 영향을 안준다\n",
    "  유니크(number of unique values)값이 너무 많고 분석에 필요없는 데이터 지운다(예를 들어 zipcode 나 url 주소)\n",
    "  카테고라이즈 해서 분석해야 할것 즉, date, region, job title 같은  변수 따로 뺴서 분석.\n",
    "  신용상태 분류나 신용등급 회귀분석에는 필요없어서 따로 분석하기 위해 따로 빼둔다.\n",
    "  결측치는 Missing 값으로 imputation.\n",
    "  각 범주형 변수에서 unique values를 그룹화하여 종속변수와의 평균값 order순으로 인코딩한다.(카데고리형도 순서형 변수로 변환)\n",
    "  one-hot-encoding은 차원이 높아지므로 비선호.\n",
    "\n",
    "\n",
    "###### 종속변수와 상관관계 높은 변수선택이나 낮은 변수 제거 그리고  종속변수와 직접적 연관있는 변수제거, 카테고리형 변수를 종속변수와의 평균값 order순으로 인코딩은 모델 설정전에 따로 해야한다. 분석할 목적과 모델에 따라 종속변수가 변하기 때문이다.\n",
    "\n",
    "\n",
    "####  Feature selection\n",
    "1.연속형변수: 종속변수와 상관관계 높은변수 선택(연속형은 피어슨 계수,순위형은 스피어만 계수) -modeling 하기전\n",
    "\n",
    "2.카테고리형 변수:인코딩으로 순위형으로 바꾸면 연속형변수처럼 처리. 분류나 회귀분석에 맞지않으면 제거.변수별 시각화 분석으로만 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object 타입변수들 unique 개수 봐서 필요없는것 삭제\n",
    "df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류나 회귀분석에 필요없는 변수 제거 (object 타입변수들 unique 개수 많은것+ 날짜변수)\n",
    "df.drop(['emp_title','url','title','zip_code','issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d'], inplace=True, axis=1, errors='ignore')\n",
    "# 위에서 분석을 위해 만든 변수들 이제 예측분석에 필요없으니 제거\n",
    "df.drop(['issue_year','issue_month','region','interest_payments'], inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap 보고 상호 상관관계 높은 변수 제거\n",
    "df.drop(['loan_amnt', 'funded_amnt_inv','int_rate','installment','total_acc','out_prncp_inv','total_pymnt_inv', 'total_rec_prncp', 'total_rec_int','collection_recovery_fee','acc_now_delinq','pub_rec_bankruptcies','last_pymnt_amnt','total_rev_hi_lim','tax_liens','sub_grade'], inplace=True, axis=1, errors='ignore')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object 타입변수들 unique 개수 많은것+ 날짜변수 삭제후\n",
    "df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preprocessing (종속변수- loan_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산이 0에 가까운 변수들 ( feature selecting 할때 사용한다. R에서 nearZeroVar 함수역할)\n",
    "# tot_coll_amt, collection_12_mths_ex_med, recoveries,total_rec_late_fee,revol_bal,pub_rec\n",
    "\n",
    "df=df.drop(['collections_12_mths_ex_med','tot_coll_amt'],axis = 1)   \n",
    "# recoveries,total_rec_late_fee를 지워야 하지만 bad loan을 나타내는 중요변수기에 삭제전에 영향을 보고 뒤에서 지울것이다.\n",
    "\n",
    "# 상관성이 똑같은 credit score이 있어서 삭제.\n",
    "df=df.drop('grade', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 변수를 인코딩 한다. 여기선 loan_condition이 문자변수라 숫자화 하기위해 loan_condition_E로 인코딩함.\n",
    "from scipy.stats import trim_mean\n",
    "# m = stats.trim_mean(train[''], 0.1) # Trim 10% at both ends of the mean 극단치 제외하기위해 mean 대신 trim_mean 사용\n",
    "\n",
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering['val'] = frame[feature].unique() #각 특성의 카테고리값을 val에 넣었다\n",
    "    ordering.index = ordering.val\n",
    "    ordering['spmean'] = frame[[feature, 'credit_score']].groupby(feature)['credit_score'].apply(trim_mean, 0.1) #각 특성의 카테고리값의 credit_score 평균을 spmean에 넣는다\n",
    "    ordering = ordering.sort_values('spmean') # credit_score 평균값이 작은 카테고리부터 sort된다.\n",
    "    ordering['ordering'] = range(1, ordering.shape[0]+1) # 1부터 카데고리수만큼 순서를 정한후\n",
    "    ordering = ordering['ordering'].to_dict()\n",
    "\n",
    "    for cat, o in ordering.items(): # 키,밸류 뽑아서\n",
    "        frame.loc[frame[feature] == cat, feature+'_E'] = o \n",
    "        \n",
    "encode(df, 'loan_condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관성이 똑같은 loan_condition_E이 있어서 삭제.\n",
    "df=df.drop('loan_condition', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop('loan_status', axis = 1) # 종속변수 'loan_condition' 와 직접적인 상관관계가 있어 제외한다.\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = [f for f in df.columns if df.dtypes[f] != 'object']\n",
    "qualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing categories imoutation\n",
    "for c in qualitative:\n",
    "    df[c] = df[c].astype('category')\n",
    "    if df[c].isnull().any():\n",
    "        df[c] = df[c].cat.add_categories(['MISSING'])\n",
    "        df[c] = df[c].fillna('MISSING')\n",
    "        # null -> missing 카데고리를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'loan_condition_E' 에서 good loan, bad loan을 분류한다.  loan_condition_E를 종속변수로 다시 변수 셀렉션을 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_condition_E'].value_counts()\n",
    "\n",
    "# order: bad(1)-good(2)-current(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['loan_condition_E']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encording\n",
    "\n",
    "from scipy.stats import trim_mean\n",
    "# m = stats.trim_mean(train[''], 0.1) # Trim 10% at both ends of the mean 극단치 제외하기위해 mean 대신 trim_mean 사용\n",
    "    \n",
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering['val'] = frame[feature].unique() #각 특성의 카테고리값을 val에 넣었다\n",
    "    ordering.index = ordering.val\n",
    "    ordering['spmean'] = frame[[feature, 'loan_condition_E']].groupby(feature)['loan_condition_E'].apply(trim_mean, 0.1) #각 특성의 카테고리값의 credit_score 평균을 spmean에 넣는다\n",
    "    ordering = ordering.sort_values('spmean') # credit_score 평균값이 작은 카테고리부터 sort된다.\n",
    "    ordering['ordering'] = range(1, ordering.shape[0]+1) # 1부터 카데고리수만큼 순서를 정한후\n",
    "    ordering = ordering['ordering'].to_dict() #딕셔너리 키밸류 셋으로 만든후\n",
    "    \n",
    "    for cat, o in ordering.items(): # 키,밸류 뽑아서\n",
    "        frame.loc[frame[feature] == cat, feature+'_E'] = o # 카테고리 이름과 변수 이름이 만나는 행렬의 밸류값 즉 credit_score평균값을 꺼낸다.\n",
    "    \n",
    "qual_encoded_C = []\n",
    "for q in qualitative:  \n",
    "    encode(df, q)\n",
    "    qual_encoded_C.append(q+'_E')\n",
    "print(qual_encoded_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩한 데이터로 대체해서 df 새로 만듬\n",
    "df_C=df[quantitative+qual_encoded_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_C['loan_condition_E'].value_counts()\n",
    "\n",
    "# order: bad(1)-good(2)-current(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_C=df_C[df_C['loan_condition_E']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_C= df_C.drop('loan_status_E', axis = 1) # 종속변수 'loan_condition_E' 와 직접적인 상관관계가 있어 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(frame, features):\n",
    "    spr = pd.DataFrame()\n",
    "    spr['feature'] = features\n",
    "    spr['spearman'] = [frame[f].corr(frame['loan_condition_E'], 'spearman') for f in features]\n",
    "    spr = spr.sort_values('spearman')\n",
    "    plt.figure(figsize=(6, 0.25*len(features)))\n",
    "    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n",
    "    \n",
    "features = quantitative + qual_encoded_C\n",
    "spearman(df_C, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.drop(['open_acc_6m','open_il_12m','open_il_24m','annual_inc_joint', \n",
    "'dti_joint','il_util','open_rv_24m','all_util','inq_last_12m','inq_fi'], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#변수 그룹별 상관관계\n",
    "#Correlation matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8)) \n",
    "plt.figure(1)\n",
    "corr = df_C[quantitative].corr()\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.figure(2)\n",
    "corr = df_C[qual_encoded_C+['loan_condition_E']].corr()\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.figure(3)\n",
    "corr = pd.DataFrame(np.zeros([len(quantitative), len(qual_encoded_C)+1]), index=quantitative, columns=qual_encoded_C+['loan_condition_E'])\n",
    "for q1 in quantitative:\n",
    "    for q2 in qual_encoded_C:\n",
    "        corr.loc[q1, q2] = df_C[q1].corr(df_C[q2])\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.tot_cur_bal.corr(df_C.home_ownership_E)\n",
    "# 3번째 히트맵. 상관관계 큰문제 없고 중요변수 아니니 넘어간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.funded_amnt.corr(df_C.total_pymnt)\n",
    "#1번째 히트맵. 높은 상관 관계이고 중요변수이면서 논리적으로도 높은 상관관계이기에 대표변수를 사용해야 한다. 대표변수는 뒤에서 정할것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### bad loan을 결정해주는 대표변수지만 분산이 없어서 지워야할 변수를 보자.  상관관계가 적다. 미래고객의 대출 상환력을 예측하는 모델에선 둘다 지우고 현재 current 상태의 고객을 예측하기 위해선 중요변수이므로 둘다 사용해야 한다. 여기선 미래고객의 예측하기위한 모델이므로 뒤에서 둘다 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.recoveries.corr(df_C.out_prncp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x=x, y=y)\n",
    "    x=plt.xticks(rotation=90)\n",
    "f = pd.melt(df, id_vars=['loan_condition_E'], value_vars=qualitative)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\n",
    "g = g.map(boxplot, \"value\", \"loan_condition_E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot 에서 loan_condition_E값으로 melt 하기 위해서는 겹쳐지니 뺸다.\n",
    "quantitative.remove('loan_condition_E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pairplot(x, y, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    ts = pd.DataFrame({'time': x, 'val': y})\n",
    "    ts = ts.groupby('time').mean()\n",
    "    ts.plot(ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "f = pd.melt(df_C, id_vars=['loan_condition_E'], value_vars=quantitative+qual_encoded_C)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\n",
    "g = g.map(pairplot, \"value\", \"loan_condition_E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### recoveries, out_prncp 는 bad loan(loan_condition=1)만 존재하기에 강력한 분류 변수이다. 하지만 인과가 바뀌었으니 지운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=df_C[['recoveries','out_prncp','loan_condition_E']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g.groupby(df_g.loan_condition_E).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 위에서 분산으로 봤을때 지워야 하지만 여기서recoveries ,out_prncp 는 bad loan(loan_condition=1)만 존재하기에 강력한 분류 변수이다. 하지만 recoveries는 인과가 바뀌었으니 지워야 하고 현재current 상태의 고객의 미래상환을 예측하려면 out_prncp는 강력한 bad loan 결정변수가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 모델에선 미래고객의 상환력을 예측하기에 둘다 지운다\n",
    "df_C.drop(['recoveries','out_prncp'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 여기서 중요한 점은 모델의 목적이다. 미래고객의 대출상환력을 예측하려면 'recoveries','out_prncp' 두변수 모두 지워야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=df_C[['total_pymnt','funded_amnt','loan_condition_E']]\n",
    "#df_p.sort_values(by='loan_condition_E')\n",
    "df_p.groupby(df_p.loan_condition_E).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loan 상태가 좋을수록(2.0) 두 집단의 funded_amnt는 비슷해도 total_pymnt 가 높다. 즉 돈을 빌려도 잘갚는다. 집단2.0을 보면 total_pymnt >unded_amnt 임을 볼수 있다. 즉 빌린돈보다 갚은돈이 많다. 이자까지 잘 갚는것을 볼수있다. 그러나 집단1.0은 절반도 갚지 못하는걸 볼수있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'funded_amnt','total_pymnt' 는 대출상태를 구분짓는 강력한 분류 변수이다. 하지만 인과가 바뀌었으니 지운다. 대출상태가 나빠서 돈을 안갚은게 아니라 돈을 안갚아서 대출상태가 나빠진것\n",
    "df_C.drop(['total_pymnt'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### funded_amnt, total_pymnt 는 앞에서 높은 상관관계가 있는 변수이면서 대출상태를 구분짓는 강력한 분류 변수이다. 하지만  total_pymnt는 인과가 바뀌었으니 지운다.  대출상태가 나빠서 돈을 안갚은게 아니라 돈을 안갚아서 대출상태가 나빠진것 이기 때문이다. funded_amnt만 봐서는 돈을 빌리는 양만으론 대출갚는 유무와 인과관계가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels \n",
    "\n",
    "Label = df_C['loan_condition_E']\n",
    "features = df_C.drop('loan_condition_E', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 훈련+검증 세트 그리고 테스트 세트로 분할\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    features, Label, random_state=0)\n",
    "# 훈련+검증 세트를 훈련 세트와 검증 세트로 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trainval, y_trainval, random_state=1)\n",
    "print(\"train 세트의 크기: {}   validate 세트의 크기: {}   test 세트의 크기:\"\n",
    "      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "\n",
    "\n",
    "# X_train, y_train  //  X_valid, y_valid  //  X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 - 데이터 표준화 작업 (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 전처리 - 데이터 표준화 작업 (scaling)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler  # 0~1 사이의 숫자로 바꾼다. 양수가 필요할때 변환\n",
    "\n",
    "sc = RobustScaler()\n",
    "transformer = sc.fit(X_train)\n",
    "transformer\n",
    "\n",
    "sc2= StandardScaler()\n",
    "transformer2 = sc2.fit(X_train)\n",
    "\n",
    "sc3= MinMaxScaler()\n",
    "transformer3 = sc3.fit(X_train)\n",
    "\n",
    "# 4분위수를 이용한 표준화 스케일 방법인 RobustScaler를 이용.이상치 영향 약화. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의할 점은 표본평균과 표본표준편차를 계산할 때 test set, validation set을 사용하면 안된다는 것이다. 표본평균/표준편차는 training set으로만 계산해야한다. Training set에서 계산한 값들로 validation set, test set을 “표준화“해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에서 training set으로 fit 시킨 평균,분산 값을 이용하여 각 세트의 독립변수(X)들을 변환시킨다. \n",
    "X_train_std=transformer.transform(X_train)\n",
    "X_valid_std=transformer.transform(X_valid)\n",
    "X_test_std=transformer.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_std2=transformer2.transform(X_train)\n",
    "X_valid_std2=transformer2.transform(X_valid)\n",
    "X_test_std2=transformer2.transform(X_test)\n",
    "\n",
    "X_train_std3=transformer3.transform(X_train)\n",
    "X_valid_std3=transformer3.transform(X_valid)\n",
    "X_test_std3=transformer3.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model  (이번 모델의 목적은 미래고객의 대출상환 정도를 예측 분류 하는것이다)\n",
    "\n",
    "https://github.com/ExcelsiorCJH/Hands-On-ML/blob/master/Chap07-Ensemble_Learning_and_Random_Forests/Chap07-Ensemble_Learning_and_Random_Forests.ipynb 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"훈련 세트 정확도: {:.3f}\".format(forest.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(forest.score(X_valid_std, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output feature importance coefficients, map them to their feature name, and sort values\n",
    "coef = pd.Series(forest.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "coef.head(25).plot(kind='bar')\n",
    "plt.title('Feature Significance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 세트의 정확도와 테스트의 정확도가 차이가 나는 것을 볼때 오버피팅이다. 과대적합을 막기 위해서 모델튜닝을 할수도 있지만 앙상블을 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 그래디언트 부스팅\n",
    "부스팅의 인기 알고리즘인 그래디언트 부스팅(Gradient Boosting)에 대해 알아보도록 하자. 그래디언트 부스팅은 '아다부스트' 처럼 전의 학습된 모델의 오차를 보완하는 방향으로 모델(분류기, 학습기)을 추가해주는 방법은 동일하다. 하지만, 그래디언트 부스팅은 아다부스트 처럼 학습단계 마다 데이터 샘플의 가중치를 업데이트 해주는 것이 아니라 학습 전단계 모델에서의 잔여 오차(residual error)에 대해 새로운 모델을 학습시키는 방법이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=42)\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_valid_std, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 튜닝으로 오버피팅 막기위해 트리의 최대 깊이를 줄여 사전 가지치기를 강하게 하거나 학습률을 낮출 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=42, max_depth=2)\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_valid_std, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=42, learning_rate=0.2)\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_valid_std, y_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=42, learning_rate=0.3,max_depth=3)\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_valid_std, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=42) #best model 선택\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "\n",
    "# Output feature importance coefficients, map them to their feature name, and sort values\n",
    "coef = pd.Series(gbrt.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "coef.head(25).plot(kind='bar')\n",
    "plt.title('Feature Significance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 테스트 (위에서 가장 성능좋은것 실행후 하자 이름이 모두 gbrt로 같기 때문.)\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train_std, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test_std, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증시보다 테스트시에 정확도가 더 오른걸 볼수있다. train,validation,test set 모두에서 비슷한 성능을 내고 있다. bias,variance 둘다 낮은 최적의 에러율을 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "\n",
    "feature_list = list(X_train.columns)\n",
    "importances = list(gbrt.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 80% of importance retained\n",
    "plt.hlines(y = 0.80, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 주요변수 2개가 67%, 4개가 81%를 설명하는 것을 볼수있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요 변수를 바탕으로 loan_condition 에 끼치는 영향을 본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=df_C[['credit_score','total_rec_late_fee','dti','term_E','annual_inc','funded_amnt','loan_condition_E']]\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_p.sort_values(by='loan_condition_E')\n",
    "df_p.groupby(df_p.loan_condition_E).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요변수의 영향을 보면 credit_score가 대출 상환에 가장 큰 영향을 보여준다. 역시 신용이 높을수록(이자가 낮을수록) 대출상환이 좋다. 그리고 납부한 연체료가 낮을수록, dti(debt/income) 가 낮을수록, 대출기간이 짧을수록,연 수익이 높을수록,빌린 금액이 작을수록 대출상환력이 높다. 특히 신용등급과 연체정도 두개 변수가 약 70%정도 모델을 설명하고 있는것을 볼수있다. 최신 current,issued 된 대출상황을 보면 특히 신용등급이 많이 높아진것을 볼수있는데 LC에서 자체적으로 데이터분석을 하였을때 같은 결론을 내리고 대출조건에 신용등급조건을 높인것을 예측할수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_p['term_E'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_p.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "gbrt.fit(X_train_std, y_train)\n",
    "y_pred = gbrt.predict(X_test_std)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm[0,0]+cm[1,1])/np.sum(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Confusion matrix에서 구한 값과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision, recall and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 정확도\n",
    "전체 샘플 중 맞게 예측한 샘플 수의 비율\n",
    "모형 트레이닝 즉 최적화에서 목적함수로 사용\n",
    "\n",
    "accuracy=TP+TN / TP+TN+FP+FN\n",
    " \n",
    "Precision 정밀도\n",
    "Positive 클래스에 속한다고 출력한 샘플 중 실제로 Positive 클래스에 속하는 샘플 수의 비율\n",
    "\n",
    "precision=TP / TP+FP\n",
    " \n",
    "Recall 재현율\n",
    "실제 Positive 클래스에 속한 샘플 중에 Positive 클래스에 속한다고 출력한 표본의 수\n",
    "\n",
    "TPR(true positive rate)\n",
    "sensitivity(민감도)\n",
    "recall=TP / TP+FN\n",
    " \n",
    "Fall-Out 위양성율\n",
    "실제 Positive 클래스에 속하지 않는 샘플 중에 Positive 클래스에 속한다고 출력한 표본의 수\n",
    "\n",
    "FPR(false positive rate)\n",
    "specificity(특이도) = 1 - fall-out\n",
    "fallout=FP / FP+TN\n",
    " \n",
    "F (beta) score\n",
    "정밀도(Precision)과 재현율(Recall)의 가중 조화 평균\n",
    "Fβ=(1+β2)(precision×recall)/(β2.precision+recall)\n",
    " \n",
    "F1 score\n",
    "beta = 1\n",
    "F1=2⋅precision⋅recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 볼때 1번집단(bad loan)을 잘 판별하지 못하였다. 그이유는 support에서도 알수 있지만 샘플의 수가 상대적으로 적고 훈련 셋을 비교하면 더욱 1번군의 샘플의 수가 적다. 2번 샘플의 양이 3배이상 많기 때문에 좋은 정확도를 보이고 있다. 그리고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_proba = gbrt.predict_proba(X_valid_std)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_valid, y_pred_proba, pos_label=2) \n",
    "# fpr, tpr, _ = metrics.roc_curve(y_valid,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_valid, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"ROC Curve, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if your data is binary, but the labels are '1' and '2', it will throw this error. So according to the documentation for the roc_curve() function from scikit-learn, you need to specify exactly which string label to use as the \"positive class\". So if your labels were 'T' and 'F' in your y_validate variable, you would do: fpr, tpr, _ = roc_curve(y_validate, status[:,1], pos_label=2). if labels are 'T' and 'F', pos_label='T'. In my case, pos_label is float so don't need ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "    \n",
    "y_pred_prob = gbrt.predict_proba(X_valid_std)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_prob, pos_label=2)\n",
    "auc = metrics.roc_auc_score(y_valid, y_pred_prob)\n",
    "\n",
    "# create plot\n",
    "plt.plot(fpr, tpr, label='ROC curve,  auc='+str(auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "_ = plt.xlabel('False Positive Rate')\n",
    "_ = plt.ylabel('True Positive Rate')\n",
    "_ = plt.title('ROC Curve')\n",
    "_ = plt.xlim([-0.02, 1])\n",
    "_ = plt.ylim([0, 1.02])\n",
    "_ = plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# save figure\n",
    "#plt.savefig('roc_curve.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC (Area Under the Curve)\n",
    "AUC는 ROC curve의 면적을 뜻한다. Fall-Out 대비 Recall 값이 클 수록 AUC가 1에 가까운 값이며 민감한 모형이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Recall Plot (이하 PR 그래프)의 경우도 ROC 와 유사한데, 주로 데이타 라벨의 분포가 심하게 불균등 할때 사용한데, 예를 들어 이상 거래 검출 시나리오의 경우 정상 거래의 비율이 비정상 거래에 비해서 압도적으로 많기 때문에 (98%, 2%) 이런 경우에는 ROC 그래프보다 PR 그래프가 분석에 더 유리하다.\n",
    "\n",
    "PR 그래프는 X 축을 Recall 값을, Y축을 Precision 값을 사용한다.\n",
    "\n",
    "\n",
    "Sensitive (Recall) = (TP) / P\n",
    "\n",
    "Precision = TP / (TP+FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve \n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# precision_score= average_precision_score(y_valid, y_pred_prob,pos_label=2)\n",
    "# precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_prob, pos_label=2)\n",
    "\n",
    "# # create plot\n",
    "# plt.plot(precision, recall, label='Precision-recall curve,  AP='+str(precision_score))\n",
    "# _ = plt.xlabel('Recall')\n",
    "# _ = plt.ylabel('Precision')\n",
    "# _ = plt.title('Precision-recall curve')\n",
    "# _ = plt.xlim([0.735, 1])\n",
    "# _ = plt.ylim([0, 1.02])\n",
    "# _ = plt.legend(loc=\"lower left\")\n",
    "\n",
    "# # save figure\n",
    "# plt.savefig('precision_recall.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_valid, y_pred_prob,pos_label=2)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_valid, y_pred_prob, pos_label=2)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision_score(y_valid, y_pred_prob,pos_label=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능판단은 ROC 그래프의 경우에는 Y=X 그래프를 기준으로 그래프 윗쪽에 있는 경우 쓸만한 모델로 판단을 했는데, PR 그래프의 경우 Base line이라는 것을 사용한다. \n",
    "\n",
    "Base line = P / (P+N) 으로 정하는데, P는 데이타에서 Positive 레이블의 수, N 은 전체 데이타의 수이다. 예를 들어 암 데이타에서 암 양성이 300개 이고, 전체 데이타가 700이면 Base line은 300/(700+300) = 0.3 이 된다.  \n",
    "\n",
    "이경우는 Base line= 36610 / 12949 + 36610 = 0.738 이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Implement Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes는 X_train 값이 양수(+)여야 해서 MinMaxScaler를 사용하여 스케일링 하였다.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_std3, y_train)\n",
    "y_pred = nb.predict(X_valid_std3)\n",
    "print(accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-Learn의 VotingClassifier를 이용해 앙상블 학습하기\n",
    "\n",
    "https://github.com/ExcelsiorCJH/Hands-On-ML/blob/master/Chap07-Ensemble_Learning_and_Random_Forests/Chap07-Ensemble_Learning_and_Random_Forests.ipynb 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hard voting\n",
    "VotingClassifier에서 voting='hard' 인 경우 각 분류기의 예측값(레이블)을 가지고 다수결 투표를 통해 최종 앙상블 예측이 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# log_clf = LogisticRegression(random_state=42)\n",
    "# rnd_clf = RandomForestClassifier(random_state=42)\n",
    "# svm_clf = SVC(random_state=42)\n",
    "# nb_clf = MultinomialNB()\n",
    "\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf),('nb', nb_clf)],\n",
    "#     voting='hard')\n",
    "# voting_clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC 가 시간이 많이 걸리는 관계로 제외후 계산, NB는 스케일링이 다르므로 제외. 대신 gredient boosting 사용\n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "gbrt_clf = GradientBoostingClassifier(random_state=42) \n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf),('gb', gbrt_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LabelEncoder 버그로 인해 생기는 DeprecationWarning 막기\n",
    "# # https://github.com/scikit-learn/scikit-learn/pull/9816\n",
    "# # https://stackoverflow.com/questions/49545947/sklearn-deprecationwarning-truth-value-of-an-array\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for clf in (log_clf, rnd_clf, svm_clf, nb_clf, voting_clf):\n",
    "#     clf.fit(X_train_std, y_train)\n",
    "#     y_pred = clf.predict(X_valid_std)\n",
    "#     print(clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, gbrt_clf, voting_clf):\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    y_pred = clf.predict(X_valid_std)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft voting\n",
    "VotingClassifier에서 voting='soft' 인 경우 각 분류기의 예측값(레이블)의 확률을 가지고 평균을 구한 뒤, 평균이 가장 높은 클래스로 최종 앙상블 예측이 이루어진다. 이러한 방법을 간접 투표(soft voting)이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_clf = LogisticRegression(random_state=42)\n",
    "# rnd_clf = RandomForestClassifier(random_state=42)\n",
    "# svm_clf = SVC(probability=True, random_state=42)\n",
    "# nb_clf= MultinomialNB()\n",
    "\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('nb', nb_clf)],\n",
    "#     voting='soft')\n",
    "# voting_clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for clf in (log_clf, rnd_clf, svm_clf, nb_clf, voting_clf):\n",
    "#     clf.fit(X_train_std, y_train)\n",
    "#     y_pred = clf.predict(X_valid_std)\n",
    "#     print(clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC 가 시간이 많이 걸리는 관계로 제외후 계산, NB는 스케일링이 다르므로 제외. 대신 gredient boosting 사용\n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "gbrt_clf = GradientBoostingClassifier(random_state=42) \n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf),('gb', gbrt_clf)],\n",
    "    voting='soft')\n",
    "# voting_clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, gbrt_clf, voting_clf):\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    y_pred = clf.predict(X_valid_std)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 배깅과 페이스팅 (Bagging and Pasting)\n",
    "위의 '투표기반 분류기'에서는 여러개의 머신러닝 알고리즘을 이용해 분류기를 만들어 앙상블 학습을 했다. 이번에 알아볼 또 다른 방법으로는 하나의 알고리즘을 사용하지만 학습 데이터셋을 랜덤하게 추출하여 모델(분류기)을 각각 다르게 학습시키는 방법이다.\n",
    "\n",
    "이렇게, 학습 데이터셋에서 랜덤하게 추출할 때, 중복을 허용하는 방식을 배깅(bagging)이라고 한다. 배깅(bagging)은 bootstrap aggregating의 줄임말이며 통계학에서는 중복을 허용한 리샘플링(resampling)을 부트스트래핑(bootstraping)이라고 한다. 이와 반대로 중복을 허용하지 않는 샘플링 방식을 페이스팅(pasting)이라고 한다.\n",
    "\n",
    "각 모델이 학습된 후에 새로운 데이터에 대해서는 '투표기반 분류기'와 동일하게 분류(classification)일 때는 최빈값(mode) 즉, 가장 많은 예측 클래스로 앙상블이 예측하며, 회귀(regression)일 경우에는 각 분류기의 예측값의 평균을 계산하여 평균값을 예측값으로 한다.\n",
    "\n",
    "각 모델은 전체 학습 데이터셋으로 학습시킨 것보다 편향되어 있지만, 앙상블을 통해 편향(bias)과 분산(variance)이 감소한다. 일반적으로 앙상블 학습은 전체 학습 데이터셋을 이용해 하나의 모델을 학습시킬 때와 비교해서 편향은 비슷하지만 분산은 줄어든다고 한다.\n",
    "\n",
    "Scikit-Learn은 배깅과 페이스팅을 간편하게 사용할 수 있도록 분류일 경우에는 BaggingClassifier를 회귀일 경우 BaggingRegressor를 제공한다.\n",
    "\n",
    "아래는 BaggingClassifier를 이용해 500개의 의사결정나무(decision tree)모델을 만들어 배깅을 적용한 앙상블 학습을 한 예제이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train_std, y_train)\n",
    "y_pred = bag_clf.predict(X_valid_std)\n",
    "\n",
    "print('Accuracy =', accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOB(Out-of-Bag) 평가\n",
    "배깅은 중복을 허용하는 리샘플링(resampling) 즉, 부트스트래핑(bootstraping) 방식이기 때문에 전체 학습 데이터셋에서 어떠한 데이터 샘플은 여러번 샘플링 되고, 또 어떠한 샘플은 전혀 샘플링 되지 않을 수가 있다. 평균적으로 학습 단계에서 전체 학습 데이터셋 중 63% 정도만 샘플링 되며, 샘플링 되지 않은 나머지 37% 데이터 샘플들을 oob(out-of-bag) 샘플이라고 한다.\n",
    "\n",
    "앙상블(배깅) 모델의 학습 단계에서는 oob 샘플이 사용되지 않기 때문에, 이러한 oob 샘플을 검증셋(validation set)이나 교차검증(cross validation)에 사용할 수 있다.\n",
    "\n",
    "Scikit-Learn에서는 BaggingClassifier의 인자인 oob_score=True로 설정하면 학습이 끝난 후 자동으로 oob 평가를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "bag_clf.fit(X_train_std, y_train)\n",
    "\n",
    "print('oob score :', bag_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래의 코드와 같이 oob 샘플에 대한 결정 함수(decision function) 값(확률값)도 확인할 수 있다\n",
    "bag_clf.oob_decision_function_[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 포레스트(Random Forest)는 배깅(bagging)을 적용한 의사결정나무(decision tree)의 앙상블이다. \n",
    "위에서 BaggingClassifier에 DecisionTreeClassifier를 인자로 넣어 줬었다. Scikit-Learn에서는 랜덤 포레스트를 간편하게 쓸 수 있도록 RandomForestClassifier를 제공 한다.\n",
    "\n",
    "아래는 RandomForestClassifier 를 이용해 모델링하고, 배깅(bagging)을 적용한 의사결정나무(decision tree)의 앙상블과 비교한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train_std, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_valid_std)\n",
    "\n",
    "# BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train_std, y_train)\n",
    "y_pred = bag_clf.predict(X_valid_std)\n",
    "\n",
    "# 두 모델의 예측 비교\n",
    "print(np.sum(y_pred == y_pred_rf) / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 엑스트라 트리 (Extremely Randomized Trees)\n",
    "랜덤 포레스트는 트리를 생성할 때, 각 노드는 랜덤하게 특성(feature)의 서브셋(자식 노드를)을 만들어 분할한다. 반면 익스트림 랜덤 트리(Extremely Randomized Trees) 또는 엑스트라 트리(Extra-Trees)는 트리를 더욱 랜덤하게 생성하기 위해 노드를 분할하는 최적의 임계값을 찾는 것이 아니라 후보 특성을 이용해 랜덤하게 분할한 다음 그 중에서 최상의 분할을 선택하는 방법이다. 랜덤 포레스트 처럼 각 노드의 특성마다 최적의 임계값을 찾는것이 아니기 때문에 엑스트라 트리가 훨씬 학습 속도가 빠르다.\n",
    "\n",
    "Scikit-Learn에서는 ExtraTreesClassifier을 이용해 엑스트라 트리를 구현할 수 있다.\n",
    "\n",
    "아래는 랜덤포레스트와 엑스트라 트리를 비교한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "extra_clf.fit(X_train_std, y_train)\n",
    "y_pred_ext = extra_clf.predict(X_valid_std)\n",
    "\n",
    "# 두 모델의 예측 비교\n",
    "print(np.sum(y_pred_rf == y_pred_ext) / len(y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 모델 성능 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomizedSearchCV 를 위한 파라미터 조건 설정\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 30, stop = 150, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 8]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use number of n_iter and number of cv respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 10,  \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_std, y_train);\n",
    "\n",
    "# n_iter, cv가 주요 하이퍼파라미터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용할이름:  X_train_std, y_train  //  X_valid_std, y_valid  //  X_test_std, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestClassifier(random_state = 42)\n",
    "base_model.fit(X_train_std, y_train)\n",
    "y_pred_tree = base_model.predict(X_valid_std)\n",
    "print('Accuracy =', accuracy_score(y_valid, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Best Random Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "\n",
    "y_pred_tree = best_random.predict(X_valid_std)\n",
    "print('Accuracy =', accuracy_score(y_valid, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Grid Search\n",
    "\n",
    "We can now perform grid search building on the result from the random search. We will test a range of hyperparameters around the best values returned by random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [30, 50],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'min_samples_split': [8,10],\n",
    "    'n_estimators': [120,150]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit( X_train_std, y_train );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Best Model from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "y_pred_tree = best_grid.predict(X_valid_std)\n",
    "print('Accuracy =', accuracy_score(y_valid, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model\n",
    "The final model from hyperparameter tuning is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_ # 위에서 valid set 평가시 성능 가장 좋았던 것을 고른다. (base_model,best_random,best_grid)\n",
    "\n",
    "print('Final Model Parameters:\\n')\n",
    "pprint(final_model.get_params())\n",
    "print('\\n')\n",
    "\n",
    "y_pred_tree = final_model.predict(X_test_std)\n",
    "print('Accuracy =', accuracy_score(y_test, y_pred_tree)) #  test set에 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set Accuracy:\",final_model.score(X_train_std,y_train))\n",
    "print(\"test set Accuracy:\",final_model.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Feature Significance\n",
    "Investigating feature importance is a relatively straight forward process:\n",
    "\n",
    "1.Out feature importance coefficients\n",
    "\n",
    "2.Map coefficients to their feature name\n",
    "\n",
    "3.Sort features in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with training data\n",
    "#clf.fit(X_train_std, y_train)\n",
    "\n",
    "# Output feature importance coefficients, map them to their feature name, and sort values\n",
    "coef = pd.Series(final_model.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "coef.head(25).plot(kind='bar')\n",
    "plt.title('Feature Significance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using grid search model hyperparameter\n",
    "\n",
    "# Output feature importance coefficients, map them to their feature name, and sort values\n",
    "coef = pd.Series(best_grid.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "coef.head(25).plot(kind='bar')\n",
    "plt.title('Feature Significance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "\n",
    "feature_list = list(X_train.columns)\n",
    "importances = list(best_grid.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
